{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b18295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e5be7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum,col,greatest,regexp_replace,expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82c1c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"skill\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55b03a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-N Elements Using orderBy and limit\n",
    "# Dataset: Sales data containing sale_id, product_name, and sales. (sale_id will be unique and product_name might be repeated for different sale_idâ€™s)\n",
    "# Task: Find the top 5 products with the highest sales.\n",
    "# Visualization: Create a bar chart in Tableau showing the top 5 products and their sales.\n",
    "data = [\n",
    "    (1, \"Titanium Cartridge\", 2500),\n",
    "    (2, \"Hand Grenade\", 1800),\n",
    "    (3, \"AK47\", 1200),\n",
    "    (4, \"Timer\", 900),\n",
    "    (5, \"RDX\", 800),\n",
    "    (6, \"Battery\", 600),\n",
    "    (7, \"AK47\", 400),\n",
    "    (8, \"Insulated Wire\", 1500),\n",
    "    (9, \"RDX\", 700),\n",
    "    (10, \"Glock\", 1300),\n",
    "    (11, \"Kevlar\", 300),\n",
    "    (12, \"Mine\", 1000),\n",
    "    (13, \"Helmet\", 400),\n",
    "    (14, \"Gloves\", 300),\n",
    "    (15, \"Paper\", 1300),\n",
    "    (16, \"RDX\", 900),\n",
    "    (17, \"Sulphur\", 1000),\n",
    "    (18, \"Mine\", 500)\n",
    "]\n",
    "\n",
    "cols = [\"ssale_id\", \"product_name\",\"sales\"]\n",
    "data_df = spark.createDataFrame(data,cols)\n",
    "result = (\n",
    "    data_df.groupBy(\"product_name\")\n",
    "    .sum(\"sales\")\n",
    "    .withColumnRenamed(\"sum(sales)\",\"total_sales\")\n",
    "    .orderBy(col(\"total_sales\").desc())\n",
    "    .limit(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "65a14a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+\n",
      "|      product_name|total_sales|\n",
      "+------------------+-----------+\n",
      "|Titanium Cartridge|       2500|\n",
      "|               RDX|       2400|\n",
      "|      Hand Grenade|       1800|\n",
      "|              AK47|       1600|\n",
      "|              Mine|       1500|\n",
      "+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "940d39f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------+--------+--------+---------+\n",
      "|employee_id|quarter1|quarter2|quarter3|quarter4|max_score|\n",
      "+-----------+--------+--------+--------+--------+---------+\n",
      "|          1|      98|      64|      98|      73|       98|\n",
      "|          2|      64|      80|      66|      53|       80|\n",
      "|          3|     100|      76|      85|      80|      100|\n",
      "|          4|      98|      52|      65|      78|       98|\n",
      "|          5|      80|      65|      55|      53|       80|\n",
      "|          6|      67|      90|      76|      87|       90|\n",
      "|          7|      76|      92|      84|      91|       92|\n",
      "|          8|      66|      96|      90|      83|       96|\n",
      "|          9|      65|      78|      50|      65|       78|\n",
      "|         10|      94|      50|      92|      84|       94|\n",
      "|         11|      87|      95|      66|      59|       95|\n",
      "|         12|      78|      68|      69|      70|       78|\n",
      "|         13|      89|      53|      99|      66|       99|\n",
      "|         14|      62|      64|      84|      79|       84|\n",
      "|         15|      76|      69|      65|      95|       95|\n",
      "|         16|      62|      60|      60|      51|       62|\n",
      "|         17|      87|      77|      91|      99|       99|\n",
      "|         18|     100|      78|      92|      71|      100|\n",
      "|         19|      88|      63|      71|      57|       88|\n",
      "|         20|      87|      84|      66|      86|       87|\n",
      "+-----------+--------+--------+--------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using greatest to Find Maximum Value Across Columns\n",
    "# Dataset: Employee performance with columns employee_id, quarter1, quarter2, quarter3, quarter4.\n",
    "# Task: Add a column showing the maximum quarterly score for each employee using the greatest function.\n",
    "# Visualization: Use Tableau to show the maximum score per employee in a heatmap.\n",
    "import random\n",
    "data = [(i, random.randint(50, 100), random.randint(50, 100), random.randint(50, 100), random.randint(50, 100)) for i in range(1, 21)]\n",
    "columns = [\"employee_id\", \"quarter1\", \"quarter2\", \"quarter3\", \"quarter4\"]\n",
    "data_df = spark.createDataFrame(data,columns)\n",
    "result = data_df.withColumn(\"max_score\",greatest(\"quarter1\",\"quarter2\",\"quarter3\",\"quarter4\"))\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "351f899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34d0514b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------------+------------------+\n",
      "|product_id|price|discount_percent|       final_price|\n",
      "+----------+-----+----------------+------------------+\n",
      "|         1|  202|           22.25|           157.055|\n",
      "|         2|  283|           17.30|           234.041|\n",
      "|         3|  212|           16.48|          177.0624|\n",
      "|         4|  402|            8.85|           366.423|\n",
      "|         5|  330|           16.84|           274.428|\n",
      "|         6|   66|           13.19|           57.2946|\n",
      "|         7|  420|           14.77|           357.966|\n",
      "|         8|  169|            6.67|          157.7277|\n",
      "|         9|  426|           28.56|          304.3344|\n",
      "|        10|  121|           16.64|          100.8656|\n",
      "|        11|   51|            7.13|           47.3637|\n",
      "|        12|  193|           16.42|161.30939999999998|\n",
      "|        13|  240|           26.50|             176.4|\n",
      "|        14|  103|           13.44|           89.1568|\n",
      "|        15|  127|           23.42| 97.25659999999999|\n",
      "|        16|   75|           14.20|             64.35|\n",
      "|        17|  343|            8.94|          312.3358|\n",
      "|        18|  384|            8.93|          349.7088|\n",
      "|        19|  234|            8.38|          214.3908|\n",
      "|        20|   73|            7.44|           67.5688|\n",
      "+----------+-----+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(i, random.randint(50, 500), f\"{random.uniform(5, 30):.2f}%\") for i in range(1, 21)]\n",
    "columns = [\"product_id\", \"price\", \"discount_percent\"]\n",
    "\n",
    "data_df = spark.createDataFrame(data,columns)\n",
    "\n",
    "data_df = data_df.withColumn(\"discount_percent\",regexp_replace(\"discount_percent\",\"%\",\"\"))\n",
    "\n",
    "result = data_df.withColumn(\"final_price\",expr(\"price - (price*discount_percent/100)\"))\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61fa81c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+------+----------+\n",
      "|   product|  region|feedback_id|rating|  comments|\n",
      "+----------+--------+-----------+------+----------+\n",
      "| Product_5|Region_3|          2|     4| Comment_2|\n",
      "| Product_7|Region_3|          5|     5| Comment_5|\n",
      "|Product_10|Region_5|          7|     5| Comment_7|\n",
      "| Product_1|Region_4|          8|     5| Comment_8|\n",
      "| Product_7|Region_1|         11|     4|Comment_11|\n",
      "| Product_6|Region_5|         12|     4|Comment_12|\n",
      "| Product_7|Region_2|         13|     4|Comment_13|\n",
      "| Product_2|Region_2|         14|     5|Comment_14|\n",
      "| Product_4|Region_1|         18|     5|Comment_18|\n",
      "| Product_9|Region_3|         22|     5|Comment_22|\n",
      "| Product_2|Region_3|         23|     5|Comment_23|\n",
      "| Product_1|Region_5|         31|     4|Comment_31|\n",
      "| Product_6|Region_2|         32|     5|Comment_32|\n",
      "| Product_5|Region_3|         34|     4|Comment_34|\n",
      "| Product_2|Region_2|         37|     4|Comment_37|\n",
      "|Product_10|Region_2|         38|     5|Comment_38|\n",
      "| Product_8|Region_4|         39|     5|Comment_39|\n",
      "| Product_5|Region_2|         42|     5|Comment_42|\n",
      "| Product_5|Region_2|         43|     5|Comment_43|\n",
      "| Product_2|Region_3|         45|     4|Comment_45|\n",
      "+----------+--------+-----------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (f\"Product_{random.randint(1, 10)}\", f\"Region_{random.randint(1, 5)}\", i, random.randint(1, 5), f\"Comment_{i}\")\n",
    "    for i in range(1, 101)\n",
    "]\n",
    "columns = [\"product\", \"region\", \"feedback_id\", \"rating\", \"comments\"]\n",
    "\n",
    "\n",
    "data_df = spark.createDataFrame(data, columns)\n",
    "result = data_df.filter(col(\"rating\").isin(4,5))\n",
    "scalar = result.count()\n",
    "result.show()\n",
    "print(scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28042b1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProduct_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, (datetime(\u001b[38;5;241m2024\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m timedelta(days\u001b[38;5;241m=\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m30\u001b[39m)))\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m), random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m500\u001b[39m))\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m      4\u001b[0m ]\n\u001b[0;32m      5\u001b[0m columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msales\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      6\u001b[0m data_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(data, columns)\n",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m----> 2\u001b[0m     (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProduct_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, (\u001b[43mdatetime\u001b[49m(\u001b[38;5;241m2024\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m timedelta(days\u001b[38;5;241m=\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m30\u001b[39m)))\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m), random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m500\u001b[39m))\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m      4\u001b[0m ]\n\u001b[0;32m      5\u001b[0m columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msales\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      6\u001b[0m data_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(data, columns)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (f\"Product_{random.randint(1, 5)}\", (datetime(2024, 1, 1) + timedelta(days=random.randint(0, 30))).strftime('%Y-%m-%d'), random.randint(100, 500))\n",
    "    for _ in range(100)\n",
    "]\n",
    "columns = [\"product_id\", \"date\", \"sales\"]\n",
    "data_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "window_spec = Window.partitionBy(\"product_id\").orderBy(\"date\").rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "\n",
    "result = data_df.withColumn(\"running_total\", sum(\"sales\").over(window_spec))\n",
    "\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abe6bf26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b59da81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10\n",
    "data = [\n",
    "    (1, \"2024-12-01\", 150.50),\n",
    "    (2, \"2024-12-02\", 50.00),\n",
    "    (1, \"2024-12-03\", 200.75),\n",
    "    (3, \"2024-12-04\", 300.20),\n",
    "    (2, \"2024-12-05\", 120.00),\n",
    "    (3, \"2024-12-06\", 95.00)\n",
    "]\n",
    "\n",
    "\n",
    "columns = [\"customer_id\", \"order_date\", \"amount\"]\n",
    "\n",
    "\n",
    "data_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "\n",
    "data_df.createOrReplaceTempView(\"customer_orders\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT customer_id , AVG(amount) from  customer_orders group by customer_id having AVG(amount)>100\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2ac3003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11\n",
    "# result = data_df.groupBy(\"store_id\").agg(sum(\"sales\").alias(\"total_sales\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57011228",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12\n",
    "data = [\n",
    "    (\"HR\", \"Manager\", 1),\n",
    "    (\"HR\", \"Executive\", 2),\n",
    "    (\"IT\", \"Manager\", 3),\n",
    "    (\"IT\", \"Developer\", 4),\n",
    "    (\"Finance\", \"Analyst\", 5),\n",
    "    (\"Finance\", \"Manager\", 6),\n",
    "    (\"IT\", \"Developer\", 7),\n",
    "    (\"HR\", \"Executive\", 8)\n",
    "]\n",
    "\n",
    "\n",
    "columns = [\"department\", \"designation\", \"employee_id\"]\n",
    "\n",
    "\n",
    "data_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "\n",
    "result = data_df.groupBy(\"department\", \"designation\").agg(count(\"employee_id\").alias(\"employee_count\"))\n",
    "\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e1154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#13\n",
    "data = [\n",
    "    (1, \"North\", 150.50),\n",
    "    (2, \"South\", 200.75),\n",
    "    (3, \"North\", 100.00),\n",
    "    (4, \"East\", 250.30),\n",
    "    (5, \"West\", 300.00),\n",
    "    (6, \"South\", 180.20)\n",
    "]\n",
    "\n",
    "\n",
    "columns = [\"customer_id\", \"region\", \"purchase_amount\"]\n",
    "\n",
    "\n",
    "data_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "\n",
    "result = data_df.groupBy(\"region\").agg(avg(\"purchase_amount\").alias(\"average_purchase_amount\"))\n",
    "\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77b93ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14\n",
    "\n",
    "data_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "\n",
    "result = data_df.groupBy(\"category\").agg(\n",
    "    sum(\"sales\").alias(\"total_sales\"),\n",
    "    avg(\"sales\").alias(\"average_sales\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a3fe6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
